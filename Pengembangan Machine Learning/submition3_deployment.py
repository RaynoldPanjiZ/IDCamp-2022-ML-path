# -*- coding: utf-8 -*-
"""submition3 - Deployment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G052A65VGi4iTDhXmn-trQbjSI4TlyQh

# Image Classification Model Deployment - CNN

**Raynold Panji Zulfiandi**

> Emotion Detection

> Dataset: https://www.kaggle.com/datasets/ananthu017/emotion-detection-fer
"""

# %tensorflow_version 2.x
import tensorflow as tf
print(tf.__version__)

# cek penggunaan GPU
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  print('GPU device not found')
else:
  print('Found GPU at: {}'.format(device_name))

"""# Data Preparation"""

!pip install -q kaggle
from google.colab import files

# upload kaggle.json
from google.colab import files
files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d ananthu017/emotion-detection-fer

!mkdir datasets
!unzip -q emotion-detection-fer.zip -d datasets
!ls datasets

!ls datasets/test

TRAINING_DIR = '/content/datasets/train/'
VALIDATION_DIR = '/content/datasets/test/'

import os
os.listdir(TRAINING_DIR), os.listdir(VALIDATION_DIR)

## cek jumlah dataset
def jum_data():
  train = []
  val = []
  lists = os.listdir(TRAINING_DIR)
  for cls in lists:
    train.append(len(os.listdir(os.path.join(TRAINING_DIR, cls))))
  for cls in lists:
    val.append(len(os.listdir(os.path.join(VALIDATION_DIR, cls))))
  return train, val, lists

def cek_data():
  chs = []
  train, val, lists = jum_data()
  
  msg="\n======================================"
  for i, cls in enumerate(lists):
    tot = train[i]+val[i]
    ch = round(tot*0.8)
    chs.append(ch-train[i])
    percen = ": "+str(80)+"% dari total ✓✓" if (ch==train[i]) else "-"
    msg=msg+f"\ntotal {cls}\t: {train[i]} ===> {ch} {percen}"
  msg=msg+"\ntotal : "+str(sum(train))
  
  msg=msg+"\n======================================"
  for i, cls in enumerate(lists):
    tot = train[i]+val[i]
    percen = ": "+str(20)+"% dari total ✓✓" if (round(tot*0.2)==val[i]) else "-"
    msg=msg+f"\ntotal {cls}\t: {val[i]} ===> {round(tot*0.2)} {percen}"
  msg=msg+"\ntotal : "+str(sum(val))

  msg=msg+"\n======================================"
  return msg, chs

print(cek_data()[0])
print(cek_data()[1])

## hapus folder disgusted
import shutil
shutil.rmtree(os.path.join(TRAINING_DIR, 'disgusted') )
shutil.rmtree(os.path.join(VALIDATION_DIR, 'disgusted'))

os.listdir(TRAINING_DIR), os.listdir(VALIDATION_DIR)
print(cek_data()[0])
print(cek_data()[1])

## pindahkan gambar agar sesuai kriteria 80/20

import random 

for i, dir_cat in enumerate(jum_data()[2]):
  source = None
  dest = None

  if (cek_data()[1][i])<0:
    source = os.path.join(TRAINING_DIR, dir_cat)
    dest = os.path.join(VALIDATION_DIR, dir_cat)
  elif cek_data()[1][i]>0:
    source = os.path.join(VALIDATION_DIR, dir_cat)
    dest = os.path.join(TRAINING_DIR, dir_cat)
  
  if source==None:
    continue
  print("\n"+source+" ====> "+dest)
  files = os.listdir(source)

  for file_name in random.sample(files, abs(cek_data()[1][i])):
    shutil.move(os.path.join(source, file_name), os.path.join(dest, "mov_"+file_name))
    print(file_name+" moved")

print(cek_data()[0])
print(cek_data()[1])

"""# Data Preprocessing"""

## Augmentasi data

img_height = 150
img_width = 150
batch_size = 35

from keras.preprocessing.image import ImageDataGenerator
training_datagen = ImageDataGenerator(
    rescale = 1.0/255.0,
    shear_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest',
)

train_generator = training_datagen.flow_from_directory(
    TRAINING_DIR, 
    class_mode='categorical', 
    target_size=(img_height, img_width), 
    batch_size=batch_size
)



validation_datagen = ImageDataGenerator(
    rescale=1./255,        
)

validation_generator = validation_datagen.flow_from_directory(      
    VALIDATION_DIR,    
    class_mode='categorical',
    target_size=(img_height, img_width),
    batch_size=batch_size
)

"""# Training"""

## build architecture
num_cls = len(train_generator.class_indices)

model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(64, (3,3), padding="same", activation="relu", input_shape=(150, 150, 3), use_bias=True),
  tf.keras.layers.MaxPooling2D(2,2),

  tf.keras.layers.Conv2D(128,(3,3), padding="same", activation="relu", use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(l=0.01)),
  # tf.keras.layers.Dropout(0.2),
  tf.keras.layers.MaxPooling2D(2,2),
  
  tf.keras.layers.Conv2D(256,(3,3), padding="same", activation="relu", use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(l=0.01)),
  # tf.keras.layers.Dropout(0.2),
  tf.keras.layers.MaxPooling2D(2,2),

  tf.keras.layers.Flatten(),
  
  tf.keras.layers.Dense(512, activation="relu", use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(l=0.01)),
  tf.keras.layers.Dropout(0.2),
  
  tf.keras.layers.Dense(num_cls, activation="softmax")
])

model.summary()

## compile model

opt_adam = tf.optimizers.Adam(learning_rate=1e-2)
opt_rms = tf.optimizers.RMSprop(learning_rate=1e-4)

model.compile(
  loss = 'categorical_crossentropy',
  optimizer = opt_rms,
  metrics = ['accuracy']
)

## define callbacks

early_stopping = tf.keras.callbacks.EarlyStopping(
  monitor = "val_accuracy",
  patience = 6,
  verbose = 0,
  mode = "auto",
  restore_best_weights=True
)

callbacks = [early_stopping]

## Train model

STEP_PER_EPOCH = train_generator.n // train_generator.batch_size
VALIDATION_STEPS = validation_generator.n // validation_generator.batch_size

with tf.device("/device:GPU:0"):
  history = model.fit(
      train_generator,
      steps_per_epoch = STEP_PER_EPOCH,
      epochs = 35,
      validation_data = validation_generator,
      validation_steps = VALIDATION_STEPS,
      verbose = 1,
      callbacks = callbacks
  )

## model evaluate

loss, acc = model.evaluate(validation_generator)
print(f"valid accuracy: {acc*100}% \nvalid loss: {loss*100}%")

"""# Plot Result"""

## Plot accuracy dan Loss

import matplotlib.pyplot as plt

loss = history.history['loss']
val_loss = history.history['val_loss']

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.plot(loss, label='Training set')
plt.plot(val_loss, label='Validation set', linestyle='--')
plt.legend()
plt.grid(linestyle='--', linewidth=1, alpha=0.5)

plt.subplot(1, 2, 2)
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.plot(acc, label='Training set')
plt.plot(val_acc, label='Validation set', linestyle='--')
plt.legend()
plt.grid(linestyle='--', linewidth=1, alpha=0.5)

plt.show()

np.argmax(model.predict(validation_generator), axis=1)

np.argmax(validation_generator.classes, axis=1)

from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report
import numpy as np

Y_pred = model.predict(validation_generator)
y_pred = np.argmax(Y_pred, axis=1)

labels_list = list(train_generator.class_indices.keys())


fig, ax = plt.subplots(figsize=(18, 6))
cm = confusion_matrix(validation_generator.classes, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_list)
disp.plot(cmap=plt.cm.Greens, ax=ax)
plt.title("==== Confusion Matrix ===== \n")
plt.show()


print("=============== Classification Report ================")
labels_test = np.argmax(validation_generator.classes, axis=1)
print(classification_report(labels_test, y_pred, target_names=labels_list))

# import numpy as np
# from sklearn.metrics import confusion_matrix, classification_report


# predictions = model.predict(X_test)
# prediction_labels = np.argmax(predictions, axis=1)
# labels_test = np.argmax(y_test, axis=1)

# labels_list = list(train_generator.class_indices.keys())


# print("================== Classification Report =====================")
# print(classification_report(labels_test, prediction_labels, target_names=label_list))
# print("===============================================================")
# print("==================== Confusing Matrix ========================")
# pd.DataFrame(confusion_matrix(labels_test, prediction_labels), index=label_list, columns=label_list)

"""# Deployment"""

## save model keras *.h5

if os.path.exists('model')==False:
  os.mkdir('model')

model.save_weights("/content/model/model_weights.h5")
model.save("/content/model/model.h5")

import warnings
warnings.filterwarnings('ignore')

## Convert Model keras ke tflite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

## save model *.tflite
with open('/content/model/model.tflite', 'wb') as f:
  f.write(tflite_model)