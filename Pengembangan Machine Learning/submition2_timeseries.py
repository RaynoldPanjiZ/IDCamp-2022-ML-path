# -*- coding: utf-8 -*-
"""submition2-TimeSeries.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/195iAFXPl90QUfpg3g7x59SIHgW70NFVV

## Time Series Forecasting - LSTM

**Raynold Panji Zulfiandi**
> Datasets: https://www.kaggle.com/datasets/utathya/electricity-consumption

## Download Dataset
"""

# install kaggle package
!pip install -q kaggle

# upload kaggle.json
from google.colab import files
files.upload()

# make directory and change permission
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

# download dataset, choose 'copy api command' from kaggle dataset
!kaggle datasets download -d utathya/electricity-consumption

!mkdir dataset
!unzip electricity-consumption.zip -d dataset
!ls dataset

"""## Data Preprocessing"""

import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from keras.layers import Dense, LSTM

data_train = pd.read_csv('dataset/train.csv')
data_train.head()

data_train.isnull().sum()

data_train['datetime']

data_train['datetime'] = pd.to_datetime(data_train['datetime'])
# data_test['datetime'] = pd.to_datetime(data_test['datetime'])

time = np.array(data_train['datetime'].values)
series = np.array(data_train['temperature'].values)
 
time_split_len = int(len(series) * 0.8)
series_train = series[:time_split_len]
series_test = series[time_split_len:]
time_train = time[:time_split_len]
time_test = time[time_split_len:]

print(f'Total Data Train : {series_train.shape[0]}')
print(f'Total Data Validation : {series_test.shape[0]}')

plt.figure(figsize=(15,5))
plt.plot(time_train, series_train, color='blue', label='train')
plt.plot(time_test, series_test, color='red', label='val')
plt.xlabel('Time')
plt.ylabel('Temperature')
plt.legend()

plt.title('Temperature average', fontsize=20);

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
  series = tf.expand_dims(series, axis=-1)
  ds = tf.data.Dataset.from_tensor_slices(series)
  ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
  ds = ds.flat_map(lambda w: w.batch(window_size + 1))
  ds = ds.shuffle(shuffle_buffer)
  ds = ds.map(lambda w: (w[:-1], w[-1:]))
  return ds.batch(batch_size).prefetch(1)

"""## Training model"""

train_set = windowed_dataset(series_train, window_size=60, batch_size=100, shuffle_buffer=1000)
test_set = windowed_dataset(series_test, window_size=60, batch_size=100, shuffle_buffer=1000)

# Mencari nilai MAE dari model < 10% skala data
MAE = (data_train['temperature'].max() - data_train['temperature'].min()) * 0.1
print(MAE)

model = tf.keras.models.Sequential([
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(60, return_sequences=True)),
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(60)),
  tf.keras.layers.Dropout(0.5),
  tf.keras.layers.Dense(64, activation="relu"),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(1),
])

ES = tf.keras.callbacks.EarlyStopping(
    monitor='val_mae',
    min_delta=0,
    patience=5,
    verbose=1,
    mode='auto'
)

optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(train_set, validation_data=test_set, epochs=50, callbacks=[ES])

if(history.history['mae'][-1] < MAE and history.history['val_mae'][-1] < MAE):
  print("\n\nMAE dibawah 10%")

"""## Plot MAE dan Loss"""

figure = plt.figure(figsize = (15, 5))

figure.add_subplot(1, 2, 1)
plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('Mean Absolute Error Plot')
plt.xlabel('Epoch')
plt.ylabel('Value')
plt.legend(['Train', 'Validation'], loc = 'upper right')

figure.add_subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Plot')
plt.xlabel('Epoch')
plt.ylabel('Value')
plt.legend(['Train', 'Validation'], loc = 'upper right')

plt.show()

## Plot Zoomed MAE and Loss
epochs_zoom = range(len(history.history['loss']))[10:]

mae_zoom = history.history['mae'][10:]
val_mae_zoom = history.history['val_mae'][10:]

loss_zoom = history.history['loss'][10:]
val_loss_zoom = history.history['val_loss'][10:]



figure = plt.figure(figsize = (15, 5))

figure.add_subplot(1, 2, 1)
plt.plot(epochs_zoom, mae_zoom, 'r')
plt.plot(epochs_zoom, val_mae_zoom, 'b')
plt.title('Mean Absolute Error Plot')
plt.xlabel('Epoch')
plt.ylabel('Value')
plt.legend(['Train', 'Validation'], loc = 'upper right')

figure.add_subplot(1, 2, 2)
plt.plot(epochs_zoom, loss_zoom, 'r')
plt.plot(epochs_zoom, val_loss_zoom, 'b')
plt.title('Loss Plot')
plt.xlabel('Epoch')
plt.ylabel('Value')
plt.legend(['Train', 'Validation'], loc = 'upper right')

plt.show()

