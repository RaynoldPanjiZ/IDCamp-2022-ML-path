# -*- coding: utf-8 -*-
"""submition1-NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r_TKIkerhQwdjsNnNV7ySE7i-7VAxQr9

# Text Classification - LSTM

**Raynold Panji Zulfiandi**

> Dataset : https://www.kaggle.com/datasets/pashupatigupta/emotion-detection-from-text

## Download Datasets
"""

# install kaggle package
!pip install -q kaggle

# upload kaggle.json
from google.colab import files
files.upload()

# make directory and change permission
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

# test kaggle dataset list
!kaggle datasets list

# download dataset, choose 'copy api command' from kaggle dataset
!kaggle datasets download -d pashupatigupta/emotion-detection-from-text

# !mkdir dataset
!unzip emotion-detection-from-text.zip -d dataset
!ls dataset

"""## Load Dataset"""

import pandas as pd
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv("dataset/tweet_emotions.csv")
df

"""## Data Preprocessing"""

df['sentiment'].value_counts()

df2 = df[df['sentiment'].isin(['surprise', 'happiness', 'sadness'])]
df2['sentiment'].value_counts()

import matplotlib.pyplot as plt

label_count = df2['sentiment'].value_counts()
label_count.plot(kind='bar', title='Count label')
plt.show()

## Over sampling data

df_sample1 = df2[df2['sentiment'] == 'happiness']
df_sample2 = df2[df2['sentiment'] == 'sadness'].sample(label_count[0], replace=True)
df_sample3 = df2[df2['sentiment'] == 'surprise'].sample(label_count[0], replace=True)

df_over = pd.concat([
    df_sample1,
    df_sample2,
    df_sample3
],axis=0)

print('Random Over Sampling')
print(df_over['sentiment'].value_counts())
df_over['sentiment'].value_counts().plot(kind='bar', title='Count label')
plt.show()

df_over

## cek jika data yang null
df_over.isnull().sum()

# delete colom yang tidak digunakan
df_new = df_over.drop(columns=['tweet_id'])
df_new

# One-hot encoding

sentiment = pd.get_dummies(df_new.sentiment)
df_new = pd.concat([df_new, sentiment], axis=1)
df_new = df_new.drop(columns='sentiment')
df_new = df_new.reset_index(drop=True)    ## reset index
df_new

label_list = df_new.columns[1:].tolist()
label_list

import nltk
nltk.download('stopwords')
nltk.download('punkt')

## Menghilangkan stopwords

from nltk.corpus import stopwords
import re
STOPWORDS = set(stopwords.words('english'))
sentences = df_new['content']

for i, sent in enumerate(sentences):
  print(f"\n|  ===> {sentences[i]}")
  for word in STOPWORDS:
      token = ' ' + word + ' '
      sent = sent.replace(token, ' ')
      sentences[i] = sent.replace(' ', ' ')  ## remove stopwords
  print(f"\n|  ===> {sentences[i]}")
  sentences[i] = re.sub(r'[@#$&][0-9A-Za-z]*', '', sentences[i])  ## replace tag
  print(f"\n|  ===> {sentences[i]}")
  print(f"\n========== Stopwords pada content ke-{i} selesai ✓✓✓ ==========\n")

sentences[0]

## ubah text jadi lower-case
df_new['content'] = sentences.apply(lambda x: x.lower())
df_new['content']

# Splitting data kalimat dan label menjadi X dan y
X = df_new['content'].values
y = df_new.drop(columns='content').values

# Split data menjadi train dan test
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)

## Tokenizer
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words=3000, oov_token='<oov>', filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{"}~\t\n')
tokenizer.fit_on_texts(X_train) 
# tokenizer.fit_on_texts(X_test)

X_seq_train = tokenizer.texts_to_sequences(X_train)
X_seq_test = tokenizer.texts_to_sequences(X_test)

max([len(x) for x in X_seq_train])

## Padding
from tensorflow.keras.preprocessing.sequence import pad_sequences

X_pad_train = pad_sequences(X_seq_train) 
X_pad_test = pad_sequences(X_seq_test)

"""## Training Model"""

num_class = len(label_list)
num_class

## Buat model arsitektur neural network

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=3000, output_dim=32),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, dropout=0.2)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(254, kernel_regularizer=tf.keras.regularizers.l2(0.001), activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(num_class, activation='softmax')
])

model.summary()

## compile model

opt_sgd = tf.keras.optimizers.SGD(learning_rate=0.001)
opt_adam = tf.keras.optimizers.Adam(learning_rate=0.001)

model.compile(loss = "categorical_crossentropy", optimizer=opt_adam, metrics=['accuracy'])

## membuat callbacks earlystop 
from timeit import default_timer as timer

EarlyStop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',
                              patience=20, mode='auto')

class myCallback(tf.keras.callbacks.Callback):
  def __init__(self, logs={}):
      self.time=[]
  def on_epoch_begin(self, epoch, logs={}):
      self.starttime = timer()
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get("accuracy") >= 0.95 and logs.get("val_accuracy") >= 0.95):
      print("accuracy melebihi!")
      self.time.append(timer()-self.starttime)
      self.model.stop_training = True
    self.time.append(timer()-self.starttime)

callbacks = [EarlyStop, myCallback()]

## Train model

num_epochs = 60
history = model.fit(
    X_pad_train, y_train, 
    epochs=num_epochs, 
    validation_data=(X_pad_test, y_test), 
    verbose=1,
    batch_size=128,
    callbacks=[callbacks]
)
print(f"\n====== waktu training: {sum(callbacks[1].time)/60} menit ======")

## evaluasi model
model.evaluate(X_pad_test, y_test)

"""## Plot loss dan akurasi"""

import matplotlib.pyplot as plt

loss = history.history['loss']
val_loss = history.history['val_loss']

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.plot(loss, label='Training set')
plt.plot(val_loss, label='Validation set', linestyle='--')
plt.legend()
plt.grid(linestyle='--', linewidth=1, alpha=0.5)

plt.subplot(1, 2, 2)
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.plot(acc, label='Training set')
plt.plot(val_acc, label='Validation set', linestyle='--')
plt.legend()
plt.grid(linestyle='--', linewidth=1, alpha=0.5)

plt.show()

"""## Plot Confusing Matrix"""

import numpy as np
from sklearn.metrics import confusion_matrix, classification_report

predictions = model.predict(X_pad_test)
prediction_labels = np.argmax(predictions, axis=1)
predictions[:5], prediction_labels[:5]

labels_test = np.argmax(y_test, axis=1)
labels_test[:5]

print("================== Classification Report =====================")
print(classification_report(labels_test, prediction_labels, target_names=label_list))
print("===============================================================")
print("==================== Confusing Matrix ========================")
pd.DataFrame(confusion_matrix(labels_test, prediction_labels), index=label_list, columns=label_list)

